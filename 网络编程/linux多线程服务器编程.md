[TOC]

# 线程安全

## 构造线程安全的对象

- 唯一要求就是在构造期间不要泄漏this指针
- 不要在构造函数中注册任何回调
- 也不要在构造函数中把this指针传给跨线程的对象
- 即便在构造函数的最后一行也不行。
- 因为在构造函数的执行期间对象还没完全初始化，如果泄漏this，则别的线程会访问这个半成品，不安全。

## 避免死锁

- 一个函数如果要锁住相同类型的多个对象，为了保证始终按相同的顺序加锁，我们可以比较mutex对象的地址，始终先加锁地址较小的Mutex

## 线程安全的observer

**智能指针**

- share_ptr控制对象的生命期，share_ptr是强引用（可以想象是用铁丝绑住堆上的对象），只要有一个指向x对象的share_ptr存在，那么该x对象就不会析构，
- week_ptr不控制对象的生命期，但是它知道对象是否还活着（想象成棉线轻轻栓着堆上的对象）。如果对象还活着，那么它可以提升为有效的share_ptr，否则提升会失败，返回一个空的share_ptr，提升行为是线程安全的。
- share_ptr，week_ptr的计数在主流平台上是原子操作，没有用锁，性能不俗
- share_ptr，week_ptr的线程安全级别于std::string和stl容器一样。
- share_ptr可以定制析构函数，这个可以利用起来。

- 让observable保存week_ptr<observer>即可

**避免循环引用的常用做法**

  - onwer拥有指向child的share_ptr，child持有指向owner的week_ptr

# 线程同步精要

## 线程同步的原则

- 首要原则是尽量最低限度共享对象，减少需要同步的场合，一个对象能不暴露给别的线程就不要暴露，如果要暴露，优先考虑immutable对象，如果实在不行才暴露可修改的对象，并用同步措施来充分保护它
- 其次是使用高级的并发编程构件，如taskQueue，Producer-Consumer Queue，CountDownLatch等等
- 最后不得已必须使用底层同步原语时，只用非递归的互斥器和条件变量，慎用读写锁，不用做信号量。
- 除了使用atomic整数之外，不自己编写lock-free代码，也不要用内核级同步原语，不凭空猜测“哪种做法性能会更好”，比如spin lock vs mutex

## 互斥器

**主要原则**

- 使用raii手法封装Mutex的创建，销毁，加锁，解锁这四个步骤。
- 只用非递归的mutex。即不可重入的mutex
- 不手工调用lock，unlock函数，一切交给栈上的Gurad对象的构造和析构函数负责。
- 在每次构造guard对象时，思考已经持有的锁，防止因加锁顺序不同而导致的死锁。

**次要原则**

- 不使用跨进程的mutex，进程间通讯只用tcp sokcet
- 加锁，解锁在同一个线程，线程a不能去解锁线程b已经锁住的mutex
- 必要的时候，使用PTHERAD_MUTEX_ERRORCHECK来排错。

**避免死锁的函数设计**

- 如果一个函数即可能在已经加锁的情况下调用，有可能在未加锁的情况下调用，那么就拆成两个函数：
- 根原来的函数同名，函数加锁，转而调用第二个函数。
- 给函数名上加上WithLockHold，不加锁，把原来的函数体搬过来

## 条件变量

**wait端使用方式**

- 必须于mutex一起使用，该布尔表达式的读写受此mutex保护
- 在Mutex已上锁的时候，才调用wait（wait执行完毕会自动重新加锁）
- 把判断布尔条件和wait放在while循环中。

**用while循环开等待条件变量，而不用if的原因**

- 不一定要在mutex已经上锁的情况下调用signal
- 在signal之前一般要修改布尔表达式
- 注意区分signal和broadcast。broadcast表明状态变化，signal表明资源可用。

## Sleep不是同步原语

**线程等待的情况**

- 等待资源可用

  那么等在select，poll，epoll_wait上，要么等在条件变量上。

- 等待进入临界区

  等在Mutex上，以便读写共享数据，

**等待一段时间的常见做法**

- 如果确实需要等待一段时间，应该往eventloop里注入一个timer，然后再timer的回调中接着干活。如果要等待某个事件，就应该采用条件变量或io事件回调，不能用sleep轮询。
- 如果多线程的安全行和效率要靠代码主动调用sleep来保证，这显然是设计出了问题。在用户态做轮询是低效的。

## 线程安全的单例模式

- **双检锁的方式**

  可以参考C++ and the Perils of Double-Checked Locking

  http://www.drdobbs.com/cpp/c-and-the-perils-of-double-checked-locki/184405772

  ```c++
  class Singleton {
  public:
  static Singleton* instance();
     ...
  private:
     static Singleton* volatile pInstance; // volatile added
     int x;
     Singleton() : x(5) {}
  };
  // from the implementation file
  Singleton* Singleton::pInstance = 0;
  Singleton* Singleton::instance() {
  if (pInstance == 0) {
     Lock lock;
     if (pInstance == 0) {
        Singleton*volatile temp = new Singleton; // volatile added
        pInstance = temp;
     }
  }
  return pInstance;
  }
  ```

  

- **用pthread_once实现**

  ```c++
  .h文件
  template<typename T>
  class Singleton
  {
  public: 
  	static T& instance(){
  		pthread_once(&m_once, &Singleton::init);
  		return *m_value;
  	}
  	
  private:
  	Singleton();
  	~Singleton();
  	static void init(){
  		m_value = new T();
  	}
  	
  private:
  	static pthread_once_t m_once;
  	static T* m_value;
  };
  
  // 必须在头文件中定义static 变量
  template<typename T>
  pthread_once_t Singleton<T>::m_once = PTHREAD_ONCE_INIT;
  
  template<typename T>
  T* Singleton<T>::m_value = NULL;
  ```

## 不要使用读写锁和信号量

**读写锁误区**

- 从正确性来讲，一个常见的错误是在持有read lock的时候修改了共享数据
- 从性能方面来说，读写锁不见得比普通mutex更高效，
- reader lock可能允许提升为writer lock，也可能不允许提升。
- 通常reader lock是可重入的，writer lock是不可重入的，但是为了防止writer饥饿，writer lock通常会组赛后来的reader locker，因此reader locker在重入的时候可能死锁。另外，在追求低延迟读写的场合也不适合用读写锁。

**信号量误区**

- 条件变量配合互斥器可以完全替代其功能，而且不容易出错，
- 信号量另一个问题就是它自己有计数值，而通常我们自己的数据结构也有长度值，这就造成了同样的信息存了两份，需要时刻保持一致，这增加了程序员的负担和出错的可能

# 多线程服务器的常用模型

## 多线程服务器的适用场合

### 常见做法

1. 运行一个单线程的进程

2. 运行一个多线程的进程

3. 运行多个单线程的进程

4. 运行多个多线程的进程

**优缺点：**

- 模式1是不可伸缩的，不能发挥多核的的计算能力

- 模式3是公认的主流模式，有两个子模式
  - 简单的把模式1的进程运行多分。

  - 主进程+work进程，如果必须绑定到一个tcp port，比如httpd+fastcgi。
- 模式2是被或多人鄙视的，难写，而且和模式3相比，没有优势

- 模式4更是千夫所指，他不但没有2和3的优点，反而汇集了二者的缺点。

### 必须适用单线程的场景

**坚持单线程的场合：**

- 程序可能fork
- 限制程序的cpu占有率

**只有单线程可以fork：多线程fork会有很多麻烦。一个程序fork以后，一般有两种行为**

- 立即执行exec()，变身为另外一个程序，如shell，inted，又比如ligthd fork出子进程，然后运行fastcgi程序。或者集群中运行在计算节点上的负责启动job的守护进程，
- 不调用exec()，继续运行当前程序，要么通过共享的文件描述符与父进程通信，协同完成任务，要么接管父进程传过来的文件描述符，独立完成工作。

### 线程程序的分类

- IO线程，这类线程的主循环是IO muiltplexing，阻塞的等在select poll, epoll_wait系统调用上，这类线程也处理定时事件，当然它的功能不只IO，有些简单的计算也可以放入其中，比如消息的编码和解码
- 计算线程，这类程序的主循环是blocking queue，阻塞的等在condition variable上，这类线程一般位于thread poll中，这种线程一般不涉及IO，一般要避免任何阻塞操作
- 第三方所用的线程，比如logging，又比如database connection。

服务器程序一般不会频繁的启动和终止线程，甚至，创建线程一般只在启动的时候调用。

### 解疑

- Linux能同时开启多少个线程？

  对于32位的Linux，一个进程的地址空间是4gb，其中用户态能访问3gb左右，而一个线程的默认栈大小是10m，那么大概可以开启300个

- 多线程能提高并发度吗？
  thread per connection不适合高并发场合，其scalability不佳，one loop per thread 的并发度足够大，且与cpu数据成正比。

- mysql的官方C api不支持异步操作，对于update,instert,delete之类只要行为不管结果的操作，我们可以用一个单独的线程来，以降低服务先线程的延迟。

## 单线程服务器常用编程模型

### reactor模型

**即non-blocking IO+ IO multiplexing模型**

**常用的有**

- lighttpd，单线程服务器（nginx与之类似，每个工作进程有一个event loop）
- libevent
- libev
- ace
-  poco c++ libraries
- java NIO，包括apache mina和netty、
- Pwisted(python)

**业务处理的基本逻辑为**：

```c++
// 示意代码
while(!done)
{
	int timeout_ms = max(1000, getNextTimedCallback());
	int retval = ::poll(fds, nfds, timeout_ms);
	if (retval < 0){
		// 错误处理，回调用户的的error handle
	} else {
		// 处理到期的timers，回调用户的timer handle
		if (retval > 0) {
			// 处理IO事件，回调用户的IO event handle
		}
	}
}
```

**对于IO密集型的应用是一个不错的选择。**

## 多线程服务器常见的模型

### 常用分类

- 每个请求一个创建一个线程，使用阻塞式IO操作
- 使用线程池，同样是阻塞式IO操作，与第一种相比，这是提高性能的措施
- 使用non-blocking IO + IO multilplexing，即Java NIO的方式
- Leader/Follower等高级模式

**推荐使用one loop per thread + non blocking 模式，原因如下：**

- event loop（也叫IO loop）用作IOmultiplexing，配合non-blocking IO和定时器	
- thread pool 用来计算，具体可以是任务队列或者生产消费者队列。

## 进程间通讯只使用tcp 

### 常用的进程通讯方式

匿名管道，具名管道，posix消息队列，共享内存，信号，sockets

### 常用的同步原语

互斥器，条件变量，读写锁，文件锁，信号量

### 原因

- 可以跨主机，
- tcp port由一个进程独占，且操作系统负责自动回收文件描述符。即使程序意外退出，也不会给系统留下垃圾。重启之后比较容易恢复，而不需要重启操作系统（用跨进程的mutex就有这个风险）
- 既然port是独占的，那么可以防止程序重复启动，后面那个进程抢不到port，自然就没法初始化。
- 连个进程通过tcp通讯，如果一个崩溃了，操作系统会关闭链接，另一个进程几乎立刻就能感知到，可以快速failover，当然应用层的心跳也是必不可少的
- 可记录，可重现，

# C++多线程系统编程精要

## 多线程和fork

- linux的fork不能再多线程中调用，因为Linxu的fork只会克隆当前线程的thread of control，不克隆其他线程，fork之后，除了当前线程之外，其他线程都消失了，也就是说不能一下子就fork出一个和父进程一样的多线程子进程。
- fork之后，子进程只有一个线程，其他线程都消失了，这就造成了一个危险的局面，其他线程可能正好位于临界区内，持有了某个锁，而他突然死亡，再也没有机会去解锁了，如果子进程试图再对同一个Mutex加锁，就会立刻死锁。
- 唯一安全的做法是：再fork之后立即调用exec()去执行另外一个程序，彻底断开子进程和父进程的练习。

## raii和fork

**不会被子进程继承的属性**

- 父进程的内存锁

- 父进程的文件锁

- 父进程d的某些定时器，setitimer，alarm，timer_creator

## 用raii包装文件描述符

- linxu的文件描述符是小整数，在程序刚启动的时候，0是标准输入，1是标准输出，2是标准错误，这时，新打开的一个文件，它的文件描述符会是3，因为Posix标准要求每次新打开的文件（含socket）的时候，必须使用当前最小可用的文件描述符号码
- 为什么不能关闭标准输出和标准出错？
  - 因为有些地反方库在特殊紧急的情况下会往stdout和stderr打印出错信息，如果我们关闭了fd=1和fd=2，这两个文件操作符有可能被网络链接占用，结果造成对方收到莫名其妙的数据。
  - 正确做法是：把stdout，stderr重定向到磁盘文件，最好不要是/dev/null，这样我们不至于丢失关键信息，

- 在非阻塞编程中，我们常常要面临这样一种场景，从某个tcp链接A收到了一个request，程序开始处理这个request处理可能需要花一定的时间，为了避免耽搁（阻塞）处理其他request，程序记住了发来request的tcp链接，在某个线程池中处理这个请求；在请求处理完以后，会把response发回tcp链接A。但是，在处理request的过程中，客户端断开了tcp链接A，而另外一个客户端刚好创建了新链接B，我们的程序不能只记住tcp链接A的文件描述符，而应该持有封装socket链接的tcpconnection对象，保证在处理request期间tcp链接A的文件描述符不会被关闭，或者持有tcpconnection对象的弱引用。这样能知道socket在处理request期间是否关闭了，fd=8到底是前生，还是今生

## 多线程遵循的原则

- 每一个文件描述符只由一个线程操作

- 一个线程可以操作多个文件描述符。但一个线程不能操作别的线程拥有的描述符，有两个例外
  - 对于磁盘文件，再必要的时候多个线程可以同时调用pread，pwrite来读写同一个文件，
  - 对于udp，由于协议本身保证消息的原子性，在适当的条件下（比如消息之间彼此独立）可以多个线程同时读写同一个udp文件描述符。

## 多线程和io

**问题点**

- 如果处理IO?

  - 操作文件描述符的系统调用本身是系统安全的，我们不必担心多个线程同时操作文件描述符会造成进程崩溃或内核崩溃
  - 但是，多个线程同时操作同一个socket描述符确实很麻烦，要考虑下面几种情况
    - 如果一个线程正在阻塞的read某个socket，而另外的线程close了此socket
    - 如果一个线程正在阻塞的accept某个listenning sokcet，而另外一个线程close了此socket
    - 一个线程正准备read某个socket，而另外一个线程clost了此socket；第三个线程恰好又open了另外一个文件描述符，其fd正好又与前面的sokcet相同，这样程序就乱套了。

  - 现在不考虑关闭文件描述符的情况，只考虑读和写，情况也不见得又多好。
    - 如果两个线程同时read同一个tcp socket，两个线程几乎同时各自收到一部分的数据，如果把数据拼成完整的消息？如何知道哪部分数据先到达
    - 如果两个线程同时write同一个tcp socket，每个线程只发出半条消息，那接收方收到数据如何处理？
    - 如果给每个tcp socket配一把锁，让同时只有一个线程读和写此socket，还不如直接始终让一同一个线程来操作此socket简单
    - 对于非阻塞Io，情况也是一样的，而且收发消息的完整性与原子性几乎不可能用锁来保证，因为这样会阻塞其他io线程。

  >
  > 如此看来，理论上只有read和write可以分到两个线程中，因为tcp socket是双向Io，问题是真的值得把read和write拆分成两个线程吗?

- 能否再多个线程中同时读写同一个socket文件描述符

- 多线程处理同一个socket也可以提高效率吗？

- 多线程可以加速磁盘io吗？

  > 多个线程read和write同一个文件也并不会提速，不仅如此，多个线程分别read和write同一个磁盘上的多个文件也不见得会提速，因为每个磁盘都又一个操作队列，多个线程的读写请求到了内核也是排队执行的。

## 多线程与signal

再多线程时代，signal的语义更为复杂，信号分为两类，发送某一线程（SIG_SEGV），发送给进程中的任意线程（SIGTERM），还要考虑掩码对信号的屏蔽等，特别是再signal handle中不能调用任何pthread函数，不能通过condition varible来通知其他线程。
**在多线程中使用signal的第一原则是：不要使用signal。包括**

- 不要用signal作为ipc手段，包括不要使用SIGUSR1等信号来触发服务端的行为，如果需要，可以采用增加监听端口的方式来实现双向的，可远程访问的进程控制。
- 不要使用基于signal实现的定时函数，包括alarm，ualarm，setitimer，timer_create，sleep，usleep。等等
- 不主动处理各种异常信号，（SIGTERM，SIGINT等）只用默认的语义：结束进程。有一个例外，SIGPIPE，服务器程序通常的做法是忽略此信号，否则如果对方断开链接，而本机继续write的话，会导致程序意外终止
- 在没有别的替代方式的情况下（比方说需要处理SIGCHID信号），把异步信号转换为同步额文件描述符事件，传统的做法是在signal handle里往一个特定的pipe写一个字节，在主程序中从这个pipe读出，从而纳入统一的io事件处理框架中，现代Linux的做法是采用signalfd（2）把信号直接转换为文件描述符事件，从而从根本上避免使用signale handle。

## 编程多线程的原则

- 线程是宝贵的，一个程序可以使用几十或者十几个线程，一台机器上不应该同时运行几百个，几千个用户线程，这会大大增加内核scheduler负担，降低整体性能。
- 线程的创建和销毁是由代价的，一个程序最好一开始就创建所需的线程，并一直反复使用，不要再运行期间反复创建，销毁线程，如果必须这么做，其频度最好降低到一分钟一次，或更低
- 每个线程应该有明确的职责，例如io线程（运行EventLoop::loop()），处理io事件，计算线程（位于ThreadPool中，负责计算等等）
- 线程之间的交互应该尽量简单，理想情况下，线程之间只用消息传递，（例如消息队列），如果必须用锁，那么最好避免再一个线程同时持有两把或者更多的锁，这样可彻底防止死锁。
- 要预先考虑清楚一个Mutable shared对象将会暴露给哪些线程，每个线程是读，还是写，读写有无可能并发进行。

## 基本线程原语的选用

**不建议适用的场景**

- pthread_rwlock，读写锁通常应该慎用，
- sem_*，避免用信号量，它的功能和条件变量重合，很容易用错
- _pthread_{cancel，kill}，程序种使用他们，则通常意味着设计出了问题。

## linux上的线程标识

**问题清单**

- Pthread_t只保证同一个进程之内，同一时刻的各个线程的id不同，不能保证同一进程之内，同一时刻先后多个线程具有不同的id，更不用说一台机器上多个进程之间的id唯一性了。
- 因此pthread_t并适合用作程序种对线程的标识符

**linux下建议的做法：使用gettid系统调用的返回值作为线程id，这么做的好处：**

- 它的类型是pid_t，其值通常是一个小整数，便于在日志中输出
- 在现代linux中，他直接标识内核的任务调度id，因此在/proc文件系统中可以轻易找到对应项，/proc/tid或/proc/pid/task/tid
- 在其他系统工具中也容易定位到具体某个线程，例如在top中，我们可以按线程列出任务，然后找出cpu使用率最高的线程id，再根据程序日志判断到底哪个线程在耗用cpu/
- 任何时刻都是全局唯一的，并且由于linux分配新的pid采取递增轮回的方法，短时间内启动的多个线程也会具有不同的id、、
- 0是非法值，因为操作系统第一个进程的Init的pid就是1.
- 可以在本线程第一次调用的时候缓存tid，这样就只会有一次系统调用。

## 线程创建和销毁的守则

**创建的法则**

- 程序库不应该在未告知的情况下创建自己的“背景线程”

  - 一旦程序中不止一个线程，就很难安全的fork了，因此库不能偷偷创建线程，如果确实有必要使用背景线程，至少应该让使用者知道。另外，如果有可能，可以让使用者在初始化库的时候传入线程池或eventloop对象，这样程序就可以统筹线程的数目和用途，用以避免优先级的任务独占某个线程。
  - 如果库提供异步回调，一定要明确说明会在哪个线程调用用户提供的回调函数，这样用户就可以知道在回调函数中能不能执行耗时的操作，会不会阻塞其他任务的执行，
  - 不要为了每个计算任务，每次请求去创建线程，一般也不会为了每个网络链接创建线程，除非并发连接数和cpu数相近，一个服务程序的线程数目应该和当前负载无关，而应该和机器cpu数目有关。

- 在程序中线程的创建最好能在初始化阶段全部完成。

- 在进入main函数之前不应该启动线程

- 尽量用相同的方式创建线程，例如moduo::thread

**销毁的法则**

- 自然死亡，从主函数返回，线程正常退出

- 非正常死亡，从线程主函数抛出异常或线程触发segfault信号等非法操作

- 自杀，在线程中调用pthread_exit来立即退出线程

- 他杀，其他线程调用pthread_cancel来强制终止某个线程

> 1. 如果确实需要强制终止一个耗时很长的计算任务，而又不想在计算期间周期性的检查某个全局标识，那么可以考虑把一部分代码fork为新的进程，这样Kill一个进程比杀本进程内的线程要安全的多。
> 2. 当然，fork的新进程和本进程的通信方式也要慎重选择，最好用文件描述符，pipe.socketpair，tcp socket来收发数据，而不要采用共享内存和跨进程的互斥锁等ipc，因为这样有死锁的可能。

## exit在C++中不是线程安全的

- exit函数在c++中的作用除了终止进程，还会析构全局对象，和已经构造完成的函数静态对象，这样有潜在的死锁情况。
- 如果确实需要主动结束线程，则可以考虑用_exit（2）系统调用，他不会试图析构全局对象，也不会执行其他清理工作，比如flush标准输出。

# 高效的多线程日志

> ​	**设计的要领**
>
> 1. 日期 时间 微妙 线程 级别 正文 源文件名：行号
>    比如：
>    20120603 08:12:12 12333Z 23514 INFO hello - test.cc::41
> 2. 尽量每条日志占用一行，这样很容易awk
> 3. 时间戳精确到微妙。每条消息都通过gettimeofday获取当前时间，这么做不会有性能损失，因为gettimeofday不是系统调用，不会陷入内核
> 4. 始终使用GMT时区（Z）,
> 5. 打印线程id，便于分析多线程程序的时序，也可以检测死锁，
> 6. 打印日志级别。在线查错时先看看有无error日志。通常可以加速定位问题
> 7. 打印源文件和行号，修复bug的时候不至于搞错对象。
> 8. 每行日志的前4个字段时固定的，以空格隔开，便于脚本解析。
> 9. 应该避免再日志格式（特别时消息id）中出现正则表达式的元字符，比如"["和"]"等等，

## 日志分类

**诊断日志**

- log4j
- logback
- slf4j
- glog
- g2log
- log4cxx
- log3cpp
- log4cplus
- Pantheios
- ezlogger

**交易日志**

- 即数据库的write-ahead log，文件系统的jouraling等，用于记录状态变更，通过回放日志可以恢复每一次修改后的状态/

## 日志记录要点

- 收到的每条内部消息的id（包括关键字段，长度，hash等）

- 收到的每条外部消息的全文

- 发出的每条消息的全文，每条消息都会有一个全局唯一的id

- 关键内部状态的变更。

- 每条日志都有时间戳

## 设计的要点

- 最好整个程序（包括主程序，程序库）都使用相同的日志库

- 程序有一个整体的日志输出，而不是各个组件有各自的日志输出

## 多线程异步日志
**线程安全**
**双缓冲**

- 准备两块buffer，A和B，前端负责往buffer A填数据（日志消息），后端负责将bufferB的数据写入文件
- 当bufferA满了以后，交换A和B，让后段将bufferA的数据写入文件，而前端往bufferB填入新的日志消息，
- 两个buffer的好处
  - 再新建日志消息的时候不必等待磁盘文件操作，也避免每条新日志都触发后端日志线程，换言之，前端不是将一条条日志消息分别传给前端，而是将多条消息拼成一个大的buffer传给后端，
  - 另外，为了及时将日志写入文件，即便bufferA未满，日志库也回每3秒执行一次上述交换写入操作

## 性能需求

> **为了实现这样的性能指标，muduo日志库的实现有几点优化措施值得一提：**
>
> - 时间戳字符串中的日期和日渐两部分时缓存的，一秒之内的多条日志只需重新格式化微妙部分，
> - 日志消息的前4段是定长的，因此可以避免再运行期间求字符串的长度
> - 线程id是预先格式化为字符串，再输出日志消息时只需简单拷贝几个字节。
> - 每行消息的源文件名部分采用了编译器计算来获得basename，避免运行期间strrchr开销。见sourceFile class，利用了gcc的内置函数。

- 每秒写几千万条日志的时候，没有明显的性能损失

- 能应对一个进程产生大量日志数据的场景。例如1gb/min

- 再多线程程序中，不造成争用

## 典型的日志文件的文件名

**logfile_test.2012060-144022.hostname.3605.log**

> 1. 第一部分Logfile_test是进程的名字，通常是main函数参数中argv[0]的basename，这样容易区分究竟是哪个服务程序的日志。必要时，还可以加入程序版本
> 2. 第二部分时文件的创建时间，这样很容易通过文件名来选择某一个时间范围内的日志。例如通配符*.20120603-14*标识2012年6月3号下午2点（GMT）左右的日志文件(s)
> 3. 第三部分是机器名称，这样即便把日志文件拷贝到别的机器上，也能追溯到其来源，
> 4. 第四部分是进程id，如果一个程序一秒内反复启动，那么每次都会生成不同的日志文件。
> 5. 第五部分是统一的后缀名.log。同样是为了便于周边配套脚本的编写。

## 功能需求

- 日志有多个级别（level）

  > 1. 比如：trace，debug，info，warn。error，fatal
  > 2. 日志的输出级别再运行时可调，这样同一个可执行文件可以分别再QA测试环境的时候输出debug级别的日志，再生产环境下输出Info级别的日志。再必要的时候，可以临时在线调整日志的整体输出级别。
  > 3. 调整日志的输出级别不需要重新编译，也不需要重启进程

- 日志有多个目的地，如文件，socket，smtp

- 日志消息的格式可配置，例如org.apache.log4j.PatternLayout

- 可以设置运行时过滤器，控制不同组件的日志消息的级别和目的地

- 本地文件为日志的目的地，那么日志文件的滚动是必须的，

  > 1. 这样可以简化日志归档的实现，
  > 2. 滚动的条件通常有两个：文件大小（例如每写满1GB就换下一个文件）和时间（例如每天零点新建一个日志文件，不论前一个文件有没写满）

# muduo编程示例

## tcp shutdown

tcp 是一个全双工协议，同一个文件描述符即可读也可写，shutdown关闭了写方向的连接，保留了读方向，这称为tcp half-close。如果直接close，那么socket_fd就不能读或者写了，

## tcp 分包的几种方式

- 消息长度固定

- 使用特殊的字符或者字符串作为消息的边界。比如http协议的\r\n

- 在每条消息的头部加上一个长度字段，比较常见

- 利用消息本身的格式来分包，比如xml的<root></root>配对，json的{....}配对（一般回用到状态机的模式）

## 应用层Buffer设计

  **​output buffer必要性**

- 如果程序想发送100kb的数据，但是在write调用中，操作系统只接受了80kb的数据（受tcp 窗口的控制），你肯定不会想在原地等待，程序应该尽快交出控制权，返回event loop，那么这20kb数据应由网络库来接管，然后注册pllout事件，一旦socket变得可写就立刻发送数据，当然，如果一次性发送不完，网络库应继续关注pollout事件，如果写完了20kb，应立马关闭pollout事件，以免造成busy loop。
- busy loop：

  **​input buffer的必要性**

- tcp是无边界的字节流协议，接收方必须处理“收到的数据尚不构成一条完整消息”和“一次性收到两条消息的数据”情况，
- 网络库在处理socket可读事件的时候，必须一次性把socket里的数据读完（从操作系统搬到应用层buffer），否则回反复触发pollin事件，造成busy-loop
- 对于不完整的数据，收到的数据需要先缓存到input buffer中。等构成一条完整的消息再通知程序业务层逻辑

## 定时器

**（计时）只使用gettimeofday来获取当前时间**

- time(2)的精度太低，以秒为单位，ftime已经废弃，clock_gettime精度最高，纳米，但其系统调用的开销比gettimeofday大
- 在x86-64平台上，gettimeofday不是系统调用，而是在用户态实现的，没有上下文切换和陷入内核的开销
- gettimeofday的分辨率是1微妙，现在的实现确实能达到这个计时精度，足以满足日常需求，

 **​（定时）只使用timerfd_*系列函数来处理定时任**

- sleep、alarm，usleep在实现时有可能用了sigalarm信号，在多线程程序中处理信号是一个很麻烦的事情，应该尽量避免，在说，如果在主程序和程序库中都用sigalar,会很麻烦
- nonasleep和clock_nonasleep是线程安全的，但是在非阻塞网络编程中，绝对不能让线程挂起的方式来等待一段时间，这样一来程序就会失去响应，正确的做法是注册一个时间回调函数。
- getitimer和timer_create也是用信号来dliver超时，在多线程程序中也会有麻烦，timer_create可以指定信号的接受方式进程还是线程，算是一个进步，不过信号处理函数能做的事情实在是有限
- timerfd_create把时间变成了一个文件描述符，该文件在定时器超时的那一刻变得可读，这样就很方便的融入了select、poll框架中，用统一的方式来处理Io事件和超时事件，
- 传统的reacotr利用select、epoll的tineout来实现定时功能，但poll和epoll_wait的精度只有毫秒，远低于timerfd_settime的定时精度。

# 分布式系统工程实践

## 可靠性浅说

**常见严重错误的处理方式**

- 如果mutex失败，直接退出进程就好了，反正程序也无法正确执行下去
- 一般的程序不必在意内存分配失败，遇到这种情况直接退出即可，一方面是程序分配内存失败之前，资源监控系统已经报警，实施负载转移，另一方面，如果真遇到bad_alloc异常，也没有特别有效的方式来应对
- 程序不必考虑自盘写完的情况，第一系统会报警。如果是关键业务，必然已经有人采取必要的措施来腾出空间。

**能随时重启进程作为程序设计目标**

- 不要使用跨进程的Mutex或semahore，也不要使用共享内存，因为程序意外终止的话，无法清理资源，特别是无法解锁。
- 不要使用父子进程共享文件描述符的方式通信（pipe2），父进程死了，子进程咋办？pipe是无法重建的。

**意外重启的常见情况及原因**。

- 服务进程本机重启----程序bug或者内存耗尽
- 机器重启----kernel bug，偶然硬件错误。
- 服务进程移机重启-----硬件或网络故障。

**如何优雅的进行重启**

- 先主动停止一个服务进程心跳

  对于短链接，关闭Listen port，不会有新请求到达
  对于长连接，客户会主动faliover到备用地址，或其他活着的服务端

- 等一段时间，直到该服务进程没有活动的请求
  
- kill并重启进程。（通常是新版本）
  
- 检查新进程的服务正常与否
  
- 依次重启服务端剩余进程，可避免终端服务

**常用升级的方式**

- 做法是迁移
- 先启动一个新版本的服务进程，然后让旧版本的服务进程停止接受新的链接。
- 把所有新请求都导向新进程，这样过一段时间后，旧版本的服务进程已经没有活动请求，就可以直接kill进程，完成升级，
- 在次升级的过程中服务不中断，

## 心跳包的设计

**应用层心跳包的必要性**

- 如果操作系统崩溃导致机器重启，没有机会发送fin分节
- 服务器硬件故障导致机器重启，也没有机会发送fin分节
- 并发连接数很高时，操作系统或进程重启，可能没有机会断开全部链接。换句话说，fin分节可能出现丢包，但这时没有机会重启。
- 网络故障，链接双方得知这一情况的唯一方案时检测心跳超时

**心跳的基本形式**

- 如果进程C依赖S，那么S应该按固定周期向C发送心跳，而C按照固定的周期检测心跳。换言之，通常是服务器主动向客户端发送心跳。、
- 心跳的检查：如果reciver最后一次收到心跳的时间与当前时间之差超过某个timeout值，那么就判断对方心跳失效，如果保守一些，可以在连续两次都失效的情况下认定sender无法提供服务。
- 在故障延迟敏感的场合，可取Tc=1s，否则可取Tc=10s
- 总结：如果最近的心跳消息的接受时间早于now-2Tc，可判断心跳失效
- 考虑到闰秒的影响，Tc小于1秒是没有意义的，因为闰秒会让两台机器的相对时差发生跳变，可能造成误报。

**心跳消息的内容**

- 应包含发送方的标识符。
- 建议包含当前负载，便于客户端进行负载均衡。
- 如果sender和receiver之间有其他消息中转进程，那么还应该在心跳消息中加上sender的发送时间，防止消息在传输过程中堆积而导致假心跳

**心跳设计的关键**

- 要在工作线程中发送，不要单独起一个“心跳线程”，为了防止工作线程死锁或者阻塞的时，还在继续发送心跳。
- 与业务消息用同一个链接，不要单独用“心跳连接”。心跳的作用之一是验证网络畅通，如果它验证的不是收发业务数据的tcp连接畅通，那么意义不大。
- 避免用tcp做业务连接，用udp发送心跳，防止一旦tcp业务连接上出现堆积而影响正常业务处理时，程序还一如即往的发送udp心跳，造成客户端误认为服务可用。

## 分布式系统中的进程标识

**ip:port:start_time:pid**

> 1. 其中start_time是64位整数。标识进程的启动时刻（utc时区，从unix epoch到现在的微秒数）
> 2. 容易保证唯一性。产生这种唯一值的成本比较低，没有用到全局服务器，不存在single point of failure
> 3. 可以知道pid，在/proc/pid在可以查到相关信息。
> 4. 具有历史意义。便于追溯
> 5. 进一步，还可以把程序的名称和版本号作为gpid的一部分，
> 6. 有了唯一的gpid，那么生成全局唯一的消息id字符串也十分简单，只要在进程内使用一个原子计数器，用计数器递增的值和gpid即可组成每个消息的全局唯一值。

## 构建易于维护的分布式程序

**在服务程序内置监控接口的必要性**

- linux的procfs可以监控内核的状态。可以在/proc目录找到进程相关的信息
- 总不能用gdb attch功能，因为会导致服务进程暂停响应

**http协议的便利性**

**服务器分类**

- 连接服务器
- 登录服务器
- 逻辑服务器

## 为系统演化做准备

- 可扩展消息的第一条原则就是避免协议的版本号
- 避免通过tcp发送C struct或使用bit fields
- 可以考虑使用json，xml，google protobuf

## 自动化回归测试

 **单元测试**

- 对于c++，一个程序只能有一个main入口，要采用单元测试，需要将功能代码做成一个Library，然后让单元测试代码（包括main）函数链接到这个library。当然，为了正常启动程序，我们还需要写一个普通的main，并link到这个library
- 对多线程的测试有点乏力

## 系统部署

 ​**切记直接覆盖正在执行的文件**

- 对于c++服务程序，如果程序在运行单中，直接覆盖原有的程序，那么可能会在一段时间内出现bus error，程序因SIGBUS而crash
- 如果程序发生core dump，那么验尸的时候必须用到产生core dump的可执行文件配合core文件，否则无法进行

# C++编译链接模型精要

## C++调用C的前提

- C++必须理解头文件中struct的定义，生成与C编译器完全相同的layout

- 采用相同的对齐算法

- 遵循C语言函数的调用约定（参数传递，返回值传递，栈帧管理等等）

## C语言的编译模型及成因

###  为什么需要预处理

```
1、编译器没法在内存中完整的表示单个源文件的抽象语法树，更不可能把整个程序放在内存中，以完成交叉引用（不同源文件的函数之间相互调用，使用外部变量等等）
2、
```

### C语言采用单遍编译

是指从头到尾扫描一遍源码，一边解析代码，一边即刻生成目标代码，在单遍编译时，编译器只能看到目前已经解析过的代码，看不到后面的代码，这意味者：

- C语言要求结构体必须先定义，才能访问其成员。否则编译器不知道结构体成员的类型和偏移量，无法立刻生成目标代码
- 对于外部变量，编译器只知道它的类型和名字，不需要知道它的地址，因此需要先声明后使用，在生成目标代码中，外部变量的地址是一个空白，留给连接器去填上
- 当编译器看到一个函数调用时，按隐式函数声明规则，编译器可以立刻生成调用函数的汇编代码（函数参数入栈，调用，获取返回值），这里唯一不能确定的就是函数的实际地址，需要连接器去填

## C++的编译模型

### 单遍编译

- 对于函数重载决议，当C++编译器读到一个函数调用语句时，它必须从目前已经看到的同名函数中选择出最佳函数，哪怕后面的代码中出现了更合适的匹配，也不能影响当前的决定，这意味着我们交换两个namespace级的函数定义在源码中的位置，那么有可能改变程序的行为。
- 对于class成员函数的一个例外，编译器总是先扫描一遍class定义，再来处理其中的成员函数，因此全部同名函数都参与重载决议。

### 前向声明适用的场景

- 定义或声明Foo*，Foo&，包含对于函数参数，返回类型，局部变量，类成员变量等等，这是因为C++的内存模型是flat的，Foo的定义无法改变Foo指针和引用的含义
- 声明一个以Foo为参数或者返回类型的函数。但是，如果代码里面调用了整个函数就需要知道Foo的定义，因为编译器要使用Foo的拷贝构造函数和析构函数，因此至少到看到他们的声明（虽然构造函数没有参数，但是有可能位于private区域）

### 不能重载的三个运算符&&、||、逗号

### 不能重载一元operator&(取值操作符)

因为一旦重载operator&，整个class就不能用前向声明了，因为和引用有冲突。

## C++链接

### debug build或者release build

- debug和release分别对应可执行文件是-o0编译，还是-o2编译

- 通常的做法是看class template的短成员函数有没被inline展开。

  ```c++
  int mian()
  {
  	std::vector<int> vi;
  	cout << vi.size();
  }
  nm ./a.out | grep size | C++filt
  0000000007ac W std::vector<int,std::alloc>::size const
  // vector<int>::size()，没有Inline展开，目标中就会出现函数的弱定义，否则没有
  ```

### inline函数

- 编译器为我们自动生成的class析构函数也是inline函数，有时候我们故意到outline，防止代码膨胀或出现编译错误

- 采用impl class定义的函数，其析构函数必须要放在cpp文件中，否则编译器将隐式声明的!~Printer() inline展开的时候无法看到impl::~impl()的声明，会报错。

  ```C++
  - .h文件：
  class Printer
  {
      public:
      Printer();
      private:
      class Impl;
      Impl* m_p;
  };
  
  .cc文件
  class Printer::Impl
  {};
  Printer::Printer()
  :m_p(new Impl)
  {}
  Printer::~Printer()
  {}
  ```

### g++内置函数

- g++中内置大量的内置函数，因此源码中出现memcpy，memset，strlen之类的函数，不一定真的会去调用Libc的库函数

### 模板实例化

- 一般的教材会告诉你，模板的定义要放在头文件里，否则就会报编译错误，其实这是错误的，会报链接错误。

- 那么有没办法把模板的实现放在库里，头文件只放头文件呢？其实也是可以的，前提是你知道模板会有哪些具体化类型，并事先显式的具现化出来。比如：

  ```
  .h文件：、
    template<typename T>
    void foo(const T&);
    .cpp文件：
    tmplate<typename T&>
    {}
  
  tmplate void foo(const int&); // 显式具体化
  tmplate void foo(const char&);  //显式具体化
  ，如果漏了这几行，仍然会报链接错误。
  ```

- 对于private成员函数模板，我们也不用在头文件中给出定义，因为用户不能调用他们，也就无法随意具现化它们，所以不会造成链接错误。

### 虚函数表

- 定义或继承了虚函数的对象中会有一个隐含成员，指向vtable的指针，即vptr，在构造和析构对象的时候，编译器生成的代码会修改这个vptr成员。

## 头文件使用规则

### 头文件组织顺序

- c对应的头文件
- c语言系统的头文件
- C++标准库头文件
- C++第三方库头文件
- 本公司基础库头文件
- 本项目的头文件

### 查看头文件的依赖来源

```c++
g++ -M -I . hello.cc 
// 用g++可以帮我们查出包含路径
```

## 库文件的组织规则

### linux有四个互不兼容的api版本

**编译出来的库不兼容**

- gcc3.0之前的版本，看例如2.95.3
- gcc3.0/gcc3.1
- gcc3.2/gcc3.3
- gcc3.4~4.4

### linux共享库比window共享的优势

- 一致的内存管理，linux动态库和应用程序共享一个heap，因此动态库分配的内存可以交给应用去释放。反之亦然
- 一致的初始化，动态库里的静态对象的初始化和程序其他地方的静态对象一样，不用特别区分对象的位置。
- 在动态库的接口中可以放心的使用class,stl,boost
- 没有dllimport，dllexport的累赘，直接Include头文件就能使用。
- dll hel的问题小得多，因为lInux允许多个版本的动态库并存，并且每个符号可以有多个版本/

# 反思C++面向对象与虚函数

## abi兼容性

### abi主要内容

- 函数参数的传递方式，比如x86-64用寄存器来传函数的前四个整数参数
- 虚函数的调用方式，通常是Vptr,vtbl机制，然后用vtbl[offset]来调用
- strcut和class的布局，通过偏移量来访问数据成员
- name mangling
- rtti和异常处理的实现

### 不兼容的情况

- 给函数增加默认值
- 增加虚函数
- 增加默认模板类型参数
- 改变enum的值，把enum color的red=3改成red=4。
- 如果客户端代码有new bar，给bar增加数据成员。
- vs编译出来的debug模式和release模式,linux不会

### 多半兼容的情况

- 增加新的class
- 增加Non-virtual成员函数或static成员函数
- 修改数成员的名称，因为生产的二进制代码是按偏移量来计算的，当然，这会造成源码级别的不兼容。

### 动态库接口的推荐做法

- 暴露的接口里面不要有虚函数，要显式声明构造函数，析构函数并且不能inline（会展开导致多个编译单元不一致），另外sizeof(Foo)==sizeof(Foo::impl*)
- 在库的实现中把调用转发给实现Foo::impl，这部分代码位于.so/.dll中，随库的升级而升级
- 如果要加入新的功能，不必通过继承来扩展，可以原地修改

## 以std::function和std::bind取代虚函数

## iostream的用途和局限性

- 输入方面。istream不适合输入带格式的数据
- 输出方面，ostream的格式化输出非常繁琐。而且写死在代码里，没有stdio那么灵活
- log方面，由于ostream没有办法在多线程中保证一行输出的完整性，建议不要直接用它来写Log,
- in-memory格式化方面，由于ostringstream会动态分配内存，它不适合性能要求较高的场景。
- 文件io方面，如果用作文本文件的输入输出，fstream有上述的缺点
- 性能方面，不是很高，

# C++经验之谈

## 不要重载全局New，delete

- 绝对不要在Library中重载::operator new()

- 在主程序中重载::operator new（）的作用不大

- 可以用替换malloc的方式来实现

  > 替换Malloc，如果需要，直接从malloc层面入手，通过LD_FRELOAD来加载一个.so，其中有malloc，free的替代实现，这样就能为C。C++服务了，

## 在单元测试中mock系统调用

### 常用做法

- 加一层适配层，不直接调用系统函数。一个C++程序只能由一个Main入口，所以要先把程序做成Library，再用单元测试代码i链接这个library，假如有一个mynetcat程序，为了编写C++单元测试，我们把它拆分成两部分，即library和Main，源文件分别为mynetcat和man.c

- 第二种方式，仿冒系统函数，在单元测试程序里实现一个自己的系统函数，在链接的时候，Linker会优先采用我们自己定义的函数（这对动态链接是成立的；如果是静态链接，会报multiple definition）错误，好在绝大多数情况下Libc都是动态链接的。

  > 但是，如果程序真的需要调用系统的connect函数怎么办？在我们自己mock connect里不能再调用connect 函数了，否则会无限递归。为了防止这样的情况，我们用dlsm(RTLD_NEXT，"connect")获得connect系统函数的真实地址，然后再通过函数指针来调用它；
  > 这种方式使用于第三方的库

- 使用ld的--wrap参数。可以方便的替换动态库里的函数。

## 采用由利于版本管理的代码格式

### 对diff友好的代码格式

- 多行注释用\\，也不要用/***/
- 一行代码只定义一个变量。
- 如果函数的参数大于3个，那么在逗号后面换行，这样每个参数就占用一行，便于diff
- 函数调用的时候i，如果参数大于3个，那么把实参分行写。
- class的初始化列表同样也遵循一行一个的原则。

### 对grep友好的代码风格

- 使用static_cast，而不是c-style cast。方便grep
- 可以使用-Wold-style-cast来帮助检测c-style casting，这样在编译时就能帮我们找到问题。

## 优秀开源的库

- goole protobub
- leveldb 
- PCRE的C++封装
- libuinet，用户态的协议栈，意味着并发链接tcp不再占用系统文件数，只占内存

